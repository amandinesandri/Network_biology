---
output: html_notebook
title: "Causal Structure Learning"
---

<https://github.com/yvan-sraka/causal-structure-learning>

# Generating random graphs

`random.graph` of `bnlearn` package help us to generate non structured random graph: <https://cran.r-project.org/web/packages/bnlearn/bnlearn.pdf>

```{r}
#install.packages("bnlearn")
library("bnlearn")
```
## Getting started with `igraph`

Here we use the `igraph` R package to create, manipulate and analyze networks:

```{r}
#install.packages("igraph")
library("igraph")
```

We might be interested in generating random graphs when analyzing real data, to judge the significance of the properties of known graphs (e.g. with empirical p-values) or to gain knowledge about the way a given network may have been created.

## The Erdös-Rényi model (random network)

The Erdös-Rényi (ER) model is the simplest model for generating random graphs, where every edge is included in the graph with a constant probability p that is independent from every other edge.

1. Generate an Erdös-Rényi graph with 4 vertices, and a probability of connection p = 1/2 :

```{r}
ER.graph = erdos.renyi.game(4, (1/2))
```

## The Watts-Strogatz model (small-world network)

The ER graphs do not have two important properties observed in many real-world networks:

- They do not generate local clustering and triadic closures. Instead, because they have a constant, random, and independent probability of two nodes being connected, ER graphs have a low clustering coeficient.
- They do not account for the formation of hubs. Formally, the degree distribution of ER graphs converges to a Poisson distribution, rather than a power law observed in many real-world, scale-free networks.

The Watts and Strogatz model was designed as the simplest possible model that addresses the first of the two limitations. It accounts for clustering while retaining the short average path lengths of the ER model.

We generate a Watts-Strogatz graph with 4 vertices:

```{r}
WS.graph = watts.strogatz.game(dim = 1, size = 4, nei = 2, p = 0.5)
```

<!--
What is the average degree ? Compare the degree distribution with the one generated by the ER model.

Read up on the transitivity method and compare the values for the ER and WS models.
-->

## The Albert-Barabsi model (scale-free network)

Many observed networks (at least approximately) fall into the class of scale-free networks, meaning that they have power-law (or scale-free) degree distributions. The Barabásifi-Albert model is one of several proposed models that generate scale-free networks. It incorporates two important general concepts: growth and preferential attachment. Both growth and preferential attachment exist widely in real networks.
Growth means that the number of nodes in the network increases over time, and Preferential attachment means that the more connected a node is, the more likely it is to receive new links.

We generate an Albert-Barabasi graph with 4 vertices:

```{r}
AB.graph = barabasi.game(4, power = 0.5, m = 2)
```

<!--
Can we guess the total number of edges? How? Verify your guess.

Give the range of the degrees in the graph. Are there any hub? Were there any hubs with the other models?

Using the function degree.distribution, plot the degree distribution of your graph and compare it to the other two models.

## Topological measures - why are they important?

Topological measures are our only tools when the graphs are too big and complex to be plotted.

Using the `igraph` functions, for the graph of your choice, find the nodes with the highes clustering coeficient, connectivity, betweenness and the lowest eccentricity, and closeness.
-->

# Causal Network Learning

In this notebook we try to hack with differents methods implemented in `CompareCausalNetworks` (<https://CRAN.R-project.org/package=CompareCausalNetworks>) R package following paramaters as describes in followig article <https://arxiv.org/abs/1706.09141>:

```{r}
#install.packages("CompareCausalNetworks")
library("CompareCausalNetworks")
```

## Librairies

```{r}
#install.packages("backShift")
#install.packages("InvariantCausalPrediction")

# Dependencies needed by pcalg on Ubuntu 18.04 LTS
#source("http://bioconductor.org/biocLite.R")
#biocLite("graph")
#biocLite("RBGL")
#biocLite("Rgraphviz")

#install.packages("pcalg") #, dependencies = TRUE)
```

This 2nd example are from `CompareCausalNetworks` GitHub repository <https://github.com/christinaheinze/CompareCausalNetworks/>:

```{r}
# From https://stackoverflow.com/questions/23050928/error-in-plot-new-figure-margins-too-large-scatter-plot
par(mar=c(1,1,1,1))

# Simulate data with connectivity matrix A with assumptions
# 1) No hidden variables
# 2) Precise location of interventions is known

if(require(backShift) & require(pcalg) & require(InvariantCausalPrediction)){
  ## simulate data
  myseed <- 1
  
  # sample size n
  n <- 10000
  
  # p=10 predictor variables and connectivity matrix A
  p <- 4
  labels <- 1:4
  
  A <- as_adj(AB.graph) # AB, ER or WS
  # can add/remove feedback by using/not using
  # A[5,2] <- 0.8 
  
  # divide data in 10 different environments
  G <- 10
  
  # simulate choose explicity intervention targets
  simResult <- simulateInterventions(n, p, A, G, intervMultiplier = 3, 
                                     noiseMult = 1, nonGauss = TRUE, hiddenVars = FALSE, 
                                     knownInterventions = TRUE, fracVarInt = 0.2, 
                                     simulateObs = TRUE, seed = myseed)
  X <- simResult$X
  environment <- simResult$environment
  interventions <- simResult$interventions
  
  # number of unique environments
  G <- length(unique(environment))
  
  ## apply selected methods given in vector 'methods'
  methods <- c("pc", "rankPc", "backShift", "fci", "mmhc")
  
  ## The complete list of methods is...
  #methods <- c("arges", "backShift", "fci", "fciplus", "ges", "gies",
  #             "hiddenICP", "ICP", "LINGAM", "mmhc", "rankArges", "rankFci",
  #             "rankGes", "rankGies", "rankPc", "rfci", "pc", "regression")

  ## Other methods (that require dependencies) are...
  # "bivariateANM", "bivariateCAM", "CAM", "RESIT"

  
  # select whether you want to run stability selection
  stability <- FALSE
  
  # arrange graphical output into a rectangular grid
  sq <- ceiling(sqrt(length(methods)+1))
  par(mfrow=c(ceiling((length(methods)+1)/sq),sq))
  
  ## plot and print true graph
  cat("\n true graph is  ------  \n" )
  print(A)
  plotGraphEdgeAttr(A, plotStabSelec = FALSE, labels = labels, 
                    thres.point = 0, main = "True graph")
  
  ## loop over all methods and compute and print/plot estimate
  for (method in methods){
    cat("\n result for method", method,"  ------  \n" )
    
    if(!stability){
      # Option 1): use this estimator as a point estimate
      Ahat <- getParents(X, environment, interventions = interventions,
                         method=method, alpha=0.1, pointConf = TRUE)
    }else{
      # Option 2): use a stability selection based estimator
      # with expected number of false positives bounded by EV=2
      Ahat <- getParentsStable(X, environment,  interventions = interventions,
                               EV=2, method=method, alpha=0.1)
    }
    
    # print and plot estimate (point estimate thresholded if numerical estimates
    # are returned)
    print(Ahat)
    if(!stability)
      plotGraphEdgeAttr(Ahat, plotStabSelec = FALSE, labels = labels, 
                        thres.point = 0.05,
                        main=paste("Point estimate for method", toupper(method)))
    else
      plotGraphEdgeAttr(Ahat, plotStabSelec = TRUE, labels = labels,
                        thres.point = 0, main = paste("Stability selection estimate for method", 
                                                      toupper(method)))
  }
}else{
  cat("\nThe packages 'backShift', 'InvariantCausalPrediction' and 'pcalg' are needed for the demo to 
 work. Please install them.")
}
```

<!--
**non-normality:** apply transformations to marginal distributions after dataset generation and / or change data generation.

**nonlinear relations:** If we consider that the distribution of the observations for a node $X$ is generated with a function of its parents: $d(x) = f(P_a (X)) + \epsilon$, one can choose $f$ among nonlinear functions (of the type $log(Y)$, $tanh(Y)$ etc ...) or even non-monotonic, but informative (like $Y^2$ for $Y$ in $[-1, 1]$). When there are several parents we can use (non) linear combinations of $Y$ or use polynomials for example.
-->